{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4S+5DPq/UqsNtKbjjErJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anas-abahaj/Sentiment-analysis/blob/main/Sentiment_analysis_using_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PNtT9g-VK-iY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None, on_bad_lines=\"skip\")\n",
        "\n",
        "# Assign column names\n",
        "df.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# Select relevant columns\n",
        "df = df[[\"target\", \"text\"]]\n",
        "\n",
        "# Map sentiment labels (Convert 0 -> negative, 4 -> positive)\n",
        "df[\"target\"] = df[\"target\"].map({0: 0, 4: 1})  # 0=negative, 1=positive\n",
        "\n",
        "# Reduce dataset size for faster processing\n",
        "data = df.sample(frac=0.25, random_state=42)  # Take 25% of data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hTKUiShMTj4g",
        "outputId": "578525fd-0065-44a5-c3ac-ccf91ee7b8bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        target                                               text\n",
              "541200       0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
              "750          0  @misstoriblack cool , i have no tweet apps  fo...\n",
              "766711       0  @TiannaChaos i know  just family drama. its la...\n",
              "285055       0  School email won't open  and I have geography ...\n",
              "705995       0                             upper airways problem "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af61dc2d-1b2c-47ef-abf4-7e82ae257992\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>541200</th>\n",
              "      <td>0</td>\n",
              "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>0</td>\n",
              "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766711</th>\n",
              "      <td>0</td>\n",
              "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285055</th>\n",
              "      <td>0</td>\n",
              "      <td>School email won't open  and I have geography ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705995</th>\n",
              "      <td>0</td>\n",
              "      <td>upper airways problem</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af61dc2d-1b2c-47ef-abf4-7e82ae257992')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af61dc2d-1b2c-47ef-abf4-7e82ae257992 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af61dc2d-1b2c-47ef-abf4-7e82ae257992');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8109444b-3c08-4d09-92da-4a9e7a1d1ded\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8109444b-3c08-4d09-92da-4a9e7a1d1ded')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8109444b-3c08-4d09-92da-4a9e7a1d1ded button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Dt4F9OnSTpdp",
        "outputId": "d802edeb-4d44-45b8-c2a8-772131e2f046"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         target                                               text\n",
              "41450         0  @brykins Splendid! I was told I looked like a ...\n",
              "355871        0          @herbadmother I'm so sorry!  that IS sad \n",
              "1251663       1  @JosieStingray Sounds like Eddie Murphy is coo...\n",
              "66109         0  http://twitpic.com/4incl - The tiny Porter pla...\n",
              "1334209       1  Im glad i got my old gameboy to work,now whene..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c966563-50cd-4ce0-bb48-73ffdb440398\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41450</th>\n",
              "      <td>0</td>\n",
              "      <td>@brykins Splendid! I was told I looked like a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355871</th>\n",
              "      <td>0</td>\n",
              "      <td>@herbadmother I'm so sorry!  that IS sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251663</th>\n",
              "      <td>1</td>\n",
              "      <td>@JosieStingray Sounds like Eddie Murphy is coo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66109</th>\n",
              "      <td>0</td>\n",
              "      <td>http://twitpic.com/4incl - The tiny Porter pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334209</th>\n",
              "      <td>1</td>\n",
              "      <td>Im glad i got my old gameboy to work,now whene...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c966563-50cd-4ce0-bb48-73ffdb440398')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c966563-50cd-4ce0-bb48-73ffdb440398 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c966563-50cd-4ce0-bb48-73ffdb440398');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-893da9d0-3ecd-443c-bb8c-6d8b69ab0851\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-893da9d0-3ecd-443c-bb8c-6d8b69ab0851')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-893da9d0-3ecd-443c-bb8c-6d8b69ab0851 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"@herbadmother I'm so sorry!  that IS sad \",\n          \"Im glad i got my old gameboy to work,now whene we go to universal,if the gameboy goes dead i can play on the ds haha \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('lenght of data is', len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XV_yWj5TuHu",
        "outputId": "63ab1e3d-5065-4228-c342-b4ab9ae38948"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght of data is 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbSN4nsZTwyQ",
        "outputId": "dc10a1c0-23e5-4a76-a6ec-9d7dae4bca61"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 400000 entries, 541200 to 1334209\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   target  400000 non-null  int64 \n",
            " 1   text    400000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 9.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(data.isnull().any(axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnTaS1IUT-yy",
        "outputId": "ee6c9aa0-505d-443e-b3c7-69789535ffd7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "UOH9uAEjUEkt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw2hLxzzUGyb",
        "outputId": "afd1a3bb-fb4c-4c03-fe0c-b9425822b547"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9WjhXyjURuV",
        "outputId": "ac5ba1fc-5df4-445a-e502-c5eea6ee0cbe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"they'd\", 'yours', 'then', 'above', \"we'd\", \"weren't\", 'ours', 'now', 'the', \"haven't\", 'these', 'does', 'myself', 'can', \"mustn't\", \"won't\", \"we'll\", 'having', 'which', 'between', 'o', \"they've\", 'until', 'her', 'about', \"hadn't\", 'yourself', 'were', \"you're\", 'being', 'other', \"they're\", 'own', 'there', 'over', 'same', \"he'd\", 'such', 'had', \"shan't\", 'y', 'under', 'each', \"he'll\", 'they', 'haven', \"you'd\", 'couldn', 'into', 'those', 'where', \"aren't\", 'ma', 'mightn', 'shouldn', \"doesn't\", \"mightn't\", 'don', 'against', 'has', \"you'll\", 'me', \"i'm\", 'did', 'wouldn', 'will', 'him', \"i've\", \"i'll\", 'was', 'been', 'do', 'themselves', 'we', 'so', 'it', 'that', 'during', \"i'd\", 'any', 'by', 'off', 'hasn', 'if', 'shan', 'should', 've', 'here', \"he's\", 'i', 'ain', 'hers', 'isn', 'too', 'hadn', 'whom', \"she'll\", 'm', 'yourselves', 'some', \"couldn't\", 'few', 'to', 'or', \"that'll\", 'your', 'won', 'all', \"it's\", 'needn', \"they'll\", 'below', \"isn't\", 'with', 'before', 'she', 'both', 'of', 'itself', \"you've\", 'mustn', \"shouldn't\", 'but', 't', 'herself', 'theirs', 'himself', 'because', \"we're\", \"should've\", 'doesn', 'an', 'be', 'he', \"hasn't\", \"it'd\", 'more', \"it'll\", \"needn't\", 'not', 're', \"don't\", 'am', \"didn't\", 'my', 'weren', 'than', 'why', 'no', 'down', 'at', 'them', 'while', 'only', 'aren', 'our', 'd', 'wasn', 'further', 'for', 'you', \"wasn't\", 'in', 'once', 's', 'most', \"she's\", 'ourselves', 'are', 'after', 'this', 'nor', 'll', 'a', 'didn', 'how', \"wouldn't\", 'out', 'his', 'doing', 'from', 'is', 'just', 'on', 'have', 'and', 'very', 'their', \"she'd\", 'through', 'up', \"we've\", 'when', 'its', 'again', 'what', 'as', 'who'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions (@user)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuations\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)\n",
        "print(data[\"cleaned_text\"].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlFoghziUXgZ",
        "outputId": "ab8a473d-2b22-4d4f-d872-85e024cbbd73"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "541200                                         ahhh hope ok\n",
            "750                                    cool tweet apps razr\n",
            "766711    know family drama lamehey next time u hang kim...\n",
            "285055    school email wont open geography stuff revise ...\n",
            "705995                                upper airways problem\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization, Stemming, Lemmatization, POS Tagging, NER, and Word Vectors"
      ],
      "metadata": {
        "id": "ceWw8Y94Uk2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NLTK laibrary**"
      ],
      "metadata": {
        "id": "Tv5wmtpYU2nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk"
      ],
      "metadata": {
        "id": "W12ULOw6UrVG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def process_nltk(text):\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]  # Stemming\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    pos_tags = pos_tag(tokens)  # POS Tagging\n",
        "    ner_tree = ne_chunk(pos_tags)  # Named Entity Recognition (NER)\n",
        "\n",
        "    named_entities = []\n",
        "    for subtree in ner_tree:\n",
        "        if hasattr(subtree, \"label\"):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            named_entities.append((entity, subtree.label()))\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"stemmed\": stemmed,\n",
        "        \"lemmatized\": lemmatized,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"ner\": named_entities\n",
        "    }\n",
        "\n",
        "# Apply function to a sample tweet\n",
        "sample_text = data[\"cleaned_text\"].iloc[2]\n",
        "nltk_results = process_nltk(sample_text)\n",
        "\n",
        "print(\"NLTK Tokenization:\", nltk_results[\"tokens\"])\n",
        "print(\"NLTK Stemming:\", nltk_results[\"stemmed\"])\n",
        "print(\"NLTK Lemmatization:\", nltk_results[\"lemmatized\"])\n",
        "print(\"NLTK POS Tags:\", nltk_results[\"pos_tags\"])\n",
        "print(\"NLTK Named Entities:\", nltk_results[\"ner\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PAKOMAVC0U",
        "outputId": "90471dbf-e7ff-458f-d443-cc605e44d6a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenization: ['know', 'family', 'drama', 'lamehey', 'next', 'time', 'u', 'hang', 'kim', 'n', 'u', 'guys', 'like', 'sleepover', 'whatever', 'ill', 'call', 'u']\n",
            "NLTK Stemming: ['know', 'famili', 'drama', 'lamehey', 'next', 'time', 'u', 'hang', 'kim', 'n', 'u', 'guy', 'like', 'sleepov', 'whatev', 'ill', 'call', 'u']\n",
            "NLTK Lemmatization: ['know', 'family', 'drama', 'lamehey', 'next', 'time', 'u', 'hang', 'kim', 'n', 'u', 'guy', 'like', 'sleepover', 'whatever', 'ill', 'call', 'u']\n",
            "NLTK POS Tags: [('know', 'VB'), ('family', 'NN'), ('drama', 'NN'), ('lamehey', 'IN'), ('next', 'JJ'), ('time', 'NN'), ('u', 'JJ'), ('hang', 'NN'), ('kim', 'NN'), ('n', 'JJ'), ('u', 'JJ'), ('guys', 'NNS'), ('like', 'IN'), ('sleepover', 'NN'), ('whatever', 'WDT'), ('ill', 'RB'), ('call', 'VBP'), ('u', 'JJ')]\n",
            "NLTK Named Entities: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using SpaCy labrairy**"
      ],
      "metadata": {
        "id": "TQGAMsaecQAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load small English model\n",
        "\n",
        "def process_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]  # Tokenization\n",
        "    lemmatized = [token.lemma_ for token in doc]  # Lemmatization\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]  # POS Tagging\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]  # Named Entity Recognition (NER)\n",
        "\n",
        "    # Word vectors (if available)\n",
        "    word_vectors = {token.text: token.vector for token in doc if token.has_vector}\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"lemmatized\": lemmatized,\n",
        "        \"pos_tags\": pos_tags,\n",
        "        \"ner\": named_entities,\n",
        "        \"word_vectors\": word_vectors\n",
        "    }\n",
        "\n",
        "# Apply function to the same sample tweet\n",
        "spacy_results = process_spacy(sample_text)\n",
        "\n",
        "print(\"SpaCy Tokenization:\", spacy_results[\"tokens\"])\n",
        "print(\"SpaCy Lemmatization:\", spacy_results[\"lemmatized\"])\n",
        "print(\"SpaCy POS Tags:\", spacy_results[\"pos_tags\"])\n",
        "print(\"SpaCy Named Entities:\", spacy_results[\"ner\"])\n",
        "print(\"SpaCy Word Vectors (first word):\", list(spacy_results[\"word_vectors\"].values())[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrsu3KC1VGYN",
        "outputId": "ca21d51f-d7c0-4272-b69f-facbb90898f7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy Tokenization: ['know', 'family', 'drama', 'lamehey', 'next', 'time', 'u', 'hang', 'kim', 'n', 'u', 'guys', 'like', 'sleepover', 'whatever', 'ill', 'call', 'u']\n",
            "SpaCy Lemmatization: ['know', 'family', 'drama', 'lamehey', 'next', 'time', 'u', 'hang', 'kim', 'n', 'u', 'guy', 'like', 'sleepover', 'whatever', 'ill', 'call', 'u']\n",
            "SpaCy POS Tags: [('know', 'VERB'), ('family', 'NOUN'), ('drama', 'NOUN'), ('lamehey', 'PROPN'), ('next', 'ADJ'), ('time', 'NOUN'), ('u', 'PRON'), ('hang', 'VERB'), ('kim', 'PROPN'), ('n', 'PROPN'), ('u', 'PROPN'), ('guys', 'NOUN'), ('like', 'ADP'), ('sleepover', 'NOUN'), ('whatever', 'PRON'), ('ill', 'ADJ'), ('call', 'NOUN'), ('u', 'NOUN')]\n",
            "SpaCy Named Entities: [('kim', 'PERSON')]\n",
            "SpaCy Word Vectors (first word): [-0.16450804 -1.483369    0.42516464  0.59294677 -0.67554724 -0.349852\n",
            "  0.9464916   0.80744666 -0.1556697   0.23997074  0.14656189  0.41497397\n",
            " -0.5950614   1.0855898  -0.47182953 -0.5323664   0.40713558 -1.5859385\n",
            "  0.70478374 -0.18128091 -0.8568525   0.29031026 -0.5109289   0.7604934\n",
            "  0.9598291  -1.0110701  -0.6334716   1.8341978  -0.434701   -0.85463655\n",
            "  0.4489659   0.8284271  -1.0665755   0.5055371   0.4554174   0.44163448\n",
            " -0.6461686  -1.444851    0.7245333  -1.4665709   1.4939287  -0.10385145\n",
            "  0.6987883  -0.46064246  0.58720887  0.96861154  1.8022908  -0.5369601\n",
            " -0.2126249  -0.6451604  -0.8525176  -0.03856304 -0.30842912  0.00801244\n",
            "  0.8410374  -0.520664   -0.72607917 -0.26728407  1.1178458  -0.47224936\n",
            " -1.1472007   1.3698968   0.86296785  0.8152851   0.38303804  0.6069647\n",
            " -0.6762471  -0.9452492  -1.2785296   1.1408074   1.4255493   0.5320354\n",
            "  0.46164685  0.3295996  -0.8397742   0.29855585 -1.0541681  -1.1594939\n",
            " -0.35960615 -0.767225   -1.2887819  -0.20160247 -0.5768076   0.06608237\n",
            " -0.35683638  1.477728   -0.82415366 -0.4755696  -0.00769971  0.04690742\n",
            "  0.35794538 -1.390697    0.9541171   0.16304144  0.39567268  1.4501832 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing with Gensim**"
      ],
      "metadata": {
        "id": "YwxyFcTuh0g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim provides built-in functions for tokenization, normalization, stemming, and lemmatization."
      ],
      "metadata": {
        "id": "KyP1RsKFh7ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords, stem_text, preprocess_string, strip_punctuation, strip_numeric\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Custom text preprocessing function\n",
        "def preprocess_gensim(text):\n",
        "    text = text.lower()  # Normalize case\n",
        "    text = remove_stopwords(text)  # Remove stopwords\n",
        "    text = strip_punctuation(text)  # Remove punctuation\n",
        "    text = strip_numeric(text)  # Remove numbers\n",
        "    tokens = simple_preprocess(text)  # Tokenization\n",
        "    stemmed = [stem_text(word) for word in tokens]  # Stemming\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"stemmed\": stemmed,\n",
        "        \"lemmatized\": lemmatized\n",
        "    }\n",
        "\n",
        "# Apply to a sample tweet\n",
        "sample_text = data[\"cleaned_text\"].iloc[0]\n",
        "gensim_results = preprocess_gensim(sample_text)\n",
        "\n",
        "print(\"Gensim Tokenization:\", gensim_results[\"tokens\"])\n",
        "print(\"Gensim Stemming:\", gensim_results[\"stemmed\"])\n",
        "print(\"Gensim Lemmatization:\", gensim_results[\"lemmatized\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlkUGFc6cYC5",
        "outputId": "e3f4ed80-afa7-4c57-bbd2-3bbb5a2f5544"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gensim Tokenization: ['ahhh', 'hope', 'ok']\n",
            "Gensim Stemming: ['ahhh', 'hope', 'ok']\n",
            "Gensim Lemmatization: ['ahhh', 'hope', 'ok']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Representation using Gensim**"
      ],
      "metadata": {
        "id": "0KJbXY3JiTGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert tweets into high-dimensional **vector representations**."
      ],
      "metadata": {
        "id": "nw8CV4BriboW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Tokenize the dataset\n",
        "data[\"gensim_tokens\"] = data[\"cleaned_text\"].apply(lambda x: simple_preprocess(x))\n",
        "\n",
        "# Create a dictionary from the tokenized tweets\n",
        "dictionary = Dictionary(data[\"gensim_tokens\"])\n",
        "\n",
        "# Convert tweets into Bag-of-Words (BoW) representation\n",
        "corpus_bow = [dictionary.doc2bow(text) for text in data[\"gensim_tokens\"]]\n",
        "\n",
        "print(\"Sample BoW Representation:\", corpus_bow[:2])  # Example of first 2 tweets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU7PqwcUiFT7",
        "outputId": "7b20f0a4-640a-404c-d7c6-8bb7db8bb41f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample BoW Representation: [[(0, 1), (1, 1), (2, 1)], [(3, 1), (4, 1), (5, 1), (6, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings using Word2Vec**"
      ],
      "metadata": {
        "id": "EaQeN-e2jVzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim supports Word2Vec and GloVe to learn word embeddings."
      ],
      "metadata": {
        "id": "xpXWw1Qbjbsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=data[\"gensim_tokens\"], vector_size=300, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Get vector for a word\n",
        "word_vector = w2v_model.wv[\"happy\"]  # Example word vector\n",
        "print(\"Word Vector for 'happy':\", word_vector[:10])  # Show first 10 values"
      ],
      "metadata": {
        "id": "Uy1tMISOjY-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c5f41e-2686-426e-f504-7b222f4d42d3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vector for 'happy': [ 0.42489234  1.1519021   0.920759    0.5596347   0.30815944 -0.49482748\n",
            " -0.4380642   0.62900376  0.8123519   0.21661776]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to convert a tweet (list of tokens) to a vector\n",
        "def tweet_to_vector(tokens, model, vector_size=300):\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv:  # Check if the word is in the vocabulary\n",
        "            vectors.append(model.wv[token])  # Get the word vector\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)  # Average the word vectors\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
        "\n",
        "# Apply the function to all tweets\n",
        "data[\"tweet_vector\"] = data[\"gensim_tokens\"].apply(lambda x: tweet_to_vector(x, w2v_model))"
      ],
      "metadata": {
        "id": "GZXTbhPbFnqV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first tweet vector\n",
        "first_tweet_vector = data[\"tweet_vector\"].iloc[0]\n",
        "print(\"First Tweet Vector (first 10 values):\", first_tweet_vector[:10])\n",
        "print(\"Shape of the vector:\", first_tweet_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdoD-CdeODq0",
        "outputId": "c267e741-aab1-4d87-8d65-1ae65b1352a6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Tweet Vector (first 10 values): [-0.22619586  0.01662495 -0.08241827 -0.04154763  0.02368674 -0.22663213\n",
            " -0.14134313  0.8606906   0.06888402 -0.21909879]\n",
            "Shape of the vector: (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all tweet vectors to a NumPy array\n",
        "tweet_vectors = np.array(data[\"tweet_vector\"].tolist())\n",
        "\n",
        "# Save to a file\n",
        "np.save(\"tweet_vectors.npy\", tweet_vectors)"
      ],
      "metadata": {
        "id": "4bvvQAk2OInO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 tweet vectors\n",
        "for i, vector in enumerate(tweet_vectors[:5]):\n",
        "    print(f\"Tweet {i+1} Vector (first 10 values):\", vector[:10])\n",
        "    print(f\"Shape of the vector:\", vector.shape)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc4oEZArOnEd",
        "outputId": "e5337621-f7aa-4f0c-8a60-52eeff692b88"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet 1 Vector (first 10 values): [-0.22619586  0.01662495 -0.08241827 -0.04154763  0.02368674 -0.22663213\n",
            " -0.14134313  0.86069059  0.06888402 -0.21909879]\n",
            "Shape of the vector: (300,)\n",
            "--------------------------------------------------\n",
            "Tweet 2 Vector (first 10 values): [ 0.06694639 -0.46550232  0.1992816   0.0148522   0.25755888 -0.07808398\n",
            " -0.13612124  0.28458598 -0.08352678 -0.09251469]\n",
            "Shape of the vector: (300,)\n",
            "--------------------------------------------------\n",
            "Tweet 3 Vector (first 10 values): [-0.03115662 -0.12817708  0.11884255 -0.12331461 -0.3552461  -0.17043146\n",
            " -0.16001394  0.67719871 -0.10042305  0.17600764]\n",
            "Shape of the vector: (300,)\n",
            "--------------------------------------------------\n",
            "Tweet 4 Vector (first 10 values): [-0.09890971  0.03893366 -0.02794758  0.282543   -0.14405398 -0.2921595\n",
            " -0.53804374  0.46554863 -0.45270666  0.26336318]\n",
            "Shape of the vector: (300,)\n",
            "--------------------------------------------------\n",
            "Tweet 5 Vector (first 10 values): [ 0.02924231 -0.24226141  0.12582476  0.14672522  0.16162394 -0.26669523\n",
            " -0.13701425  0.30254319  0.02624918  0.04335694]\n",
            "Shape of the vector: (300,)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Machine Learning Algorithms for Sentiment Analysis"
      ],
      "metadata": {
        "id": "15cslbK6POPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **Prepare Data for ML Models**\n",
        "\n"
      ],
      "metadata": {
        "id": "j0Ur7_RqPRRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and labels (y)\n",
        "X = tweet_vectors  # Gensim Word2Vec vectors\n",
        "y = data[\"target\"].values  # Labels (0: Negative, 1: Positive)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7toZWpZRO0p7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx4jO_KyRDfP",
        "outputId": "3704c653-b7c0-405d-8777-b075a9e1a0e8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Results:\n",
            "Accuracy: 0.7466416666666666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.73      0.74     59893\n",
            "           1       0.74      0.76      0.75     60107\n",
            "\n",
            "    accuracy                           0.75    120000\n",
            "   macro avg       0.75      0.75      0.75    120000\n",
            "weighted avg       0.75      0.75      0.75    120000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"SVM Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "id": "Y6kxRlCTTJjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Random Forest Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "9tqirnL2TOzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}